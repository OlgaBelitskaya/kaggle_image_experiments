# -*- coding: utf-8 -*-
"""image-generator-4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_OBL4MIaHnHCKrLjS8s4M2i6sI0p0Q_F

<p>Based on </p>[`"Deep Convolutional GAN (DCGAN) with MNIST" by Naoki Shibuya`](https://github.com/naokishibuya/deep-learning/blob/master/python/dcgan_mnist.ipynb)
<p>Online Version</p>[`"Image Generator of Digits"`](https://olgabelitskaya.github.io/kaggle_image_generator.html)
## ✒️ Styling, Libraries, and Helpful Functions
"""

# Commented out IPython magic to ensure Python compatibility.
# %%html
# <style> 
# h2,p {color:#196fc6; text-shadow:4px 4px 4px #ccc;} 
# span {color:midnightblue; text-shadow:4px 4px 4px #aaa;}
# div.output_prompt {color:#196fc6;}; div.input_prompt {color:crimson;} 
# div.output_area pre,div.output_subarea {font-size:15px; color:#196fc6;}
# div.output_stderr pre {background-color:ghostwhite;}
# </style>

import warnings; warnings.filterwarnings("ignore")
import numpy as np,pandas as pd,tensorflow as tf
import cv2,pylab as pl,keras as ks
from keras.layers import Input,Dense,Activation,Reshape
from keras.layers import BatchNormalization,Flatten
from keras.layers import Conv2D,Conv2DTranspose,GlobalMaxPooling2D
from keras.models import Sequential
from keras.optimizers import Adam,Nadam
from keras import backend
from keras.layers.advanced_activations import PReLU,LeakyReLU
np.set_printoptions(precision=8); rn=np.random.randint(5000)
from keras import __version__
print('keras version:',__version__)
print('tensorflow version:',tf.__version__)

def preprocess(x):    
    x=(x-.5)*2
    return np.clip(x,-1,1)
def deprocess(x):
    x=(x/2+.5)*255
#    np.place(x,x>220,255)
    x=np.clip(x,0,255)
    x=np.uint8(x)
    return x.reshape(28,28)
def latent_samples(n_samples,sample_size):
    return np.random.normal(loc=0,scale=1,
                            size=(n_samples,sample_size))
def trainable(model,trainable):
    for layer in model.layers:
        layer.trainable=trainable
def real_fake_labels(size):
    return np.ones([size,1]),np.zeros([size,1])
def display_images(generated_images):
    n_images=len(generated_images)
    rows=4; cols=n_images//rows    
    pl.figure(figsize=(cols,rows))
    for i in range(n_images):
        img=deprocess(generated_images[i])
        pl.subplot(rows,cols,i+1)
        pl.imshow(img,cmap=pl.cm.bone)
        pl.xticks([]); pl.yticks([])
    pl.tight_layout(); pl.show()
def display_loss(losses):
    losses=np.array(losses)        
    pl.figure(figsize=(12,5))
    pl.plot(losses.T[0],'-o',c='#196fc6',lw=1,label='Discriminator')
    pl.plot(losses.T[1],'-o',c='crimson',lw=1,label='Generator')
    pl.title("Training Loss Functions"); pl.legend()

"""## ✒️ Data Loading & Preprocessing"""

df_train=pd.read_csv("../input/train.csv")
id_images=["%s%s" %("pixel",pixel_no) for pixel_no in range(0,784)]
train_images=np.array(df_train[id_images])
train_images=train_images.astype('float32').reshape(-1,784)
pl.imshow(np.squeeze(train_images[rn].reshape(28,28)),
          cmap=pl.cm.bone);pl.show()
train_images.shape

df_test=pd.read_csv("../input/test.csv")
id_images=["%s%s" %("pixel",pixel_no) for pixel_no in range(0,784)]
test_images=np.array(df_test[id_images])
test_images=test_images.astype('float32').reshape(-1,784)
test_images.shape

np.argmax(train_images)/255

latent_sample784=latent_samples(1,784)
pl.imshow(np.squeeze(latent_sample784).reshape(28,28),
          cmap=pl.cm.bone); pl.show()

X_train_real=preprocess(train_images)
X_test_real=preprocess(test_images)
display_images(X_train_real[:32])
X_train_real=X_train_real.reshape(-1,28,28,1)
X_test_real=X_test_real.reshape(-1,28,28,1)

"""## ✒️ Keras DCGAN"""

def dcgan_generator(input_size,leaky_alpha):
    return Sequential([
        Dense(784,input_shape=(input_size,)),
        Reshape(target_shape=(7,7,16)),
        BatchNormalization(),
        LeakyReLU(alpha=leaky_alpha),       
        Conv2DTranspose(256,kernel_size=5,strides=2,padding='same'), 
        LeakyReLU(alpha=leaky_alpha),
        Conv2DTranspose(1,kernel_size=5,strides=2,padding='same'),
        Activation('tanh')])
dcgan_generator_example=dcgan_generator(784,.01)
dcgan_generator_example.summary()

def dcgan_discriminator(leaky_alpha):
    return Sequential([        
        Conv2D(256,kernel_size=5,strides=2,
               padding='same',input_shape=(28,28,1)),
        LeakyReLU(alpha=leaky_alpha),
        Conv2D(16,kernel_size=5,strides=2,padding='same'),   
        BatchNormalization(),
        LeakyReLU(alpha=leaky_alpha),        
        Flatten(),
        Dense(784),
        BatchNormalization(),
        LeakyReLU(alpha=leaky_alpha),        
        Dense(1),
        Activation('sigmoid')])
dcgan_discriminator_example=dcgan_discriminator(.01)
dcgan_discriminator_example.summary()

def dcgan(sample_size,leaky_alpha,
          g_learning_rate,g_beta_1,
          d_learning_rate,d_beta_1):
    backend.clear_session()    
    generator=dcgan_generator(sample_size,leaky_alpha)
    discriminator=dcgan_discriminator(leaky_alpha)
    discriminator.compile(optimizer=Adam(lr=d_learning_rate,beta_1=d_beta_1), 
                          loss='binary_crossentropy')
    gan=Sequential([generator,discriminator])
    gan.compile(optimizer=Adam(lr=g_learning_rate,beta_1=g_beta_1), 
                loss='binary_crossentropy')    
    return gan,generator,discriminator

def dcgan_train(g_learning_rate,g_beta_1,
                d_learning_rate,d_beta_1,
                leaky_alpha,smooth=.1,
                sample_size=128,epochs=100,
                batch_size=128,valid_size=32,
                show_details=True):    
    # labels for the batch size and the valid size
    y_train_real,y_train_fake=real_fake_labels(batch_size)
    y_valid_real,y_valid_fake=real_fake_labels(valid_size)
    # create a GAN, a generator and a discriminator
    gan,generator,discriminator=dcgan(sample_size,leaky_alpha,
                                      g_learning_rate,g_beta_1,
                                      d_learning_rate,d_beta_1)
    losses=[]
    for e in range(epochs):
        for i in range(len(X_train_real)//batch_size):
            # real MNIST digit images
            X_batch_real=X_train_real[i*batch_size:(i+1)*batch_size]
            # latent samples and the generated digit images
            batch_latent_samples=latent_samples(batch_size,sample_size)
            X_batch_fake=generator.predict_on_batch(batch_latent_samples)
            # train the discriminator to detect real and fake images
            trainable(discriminator,True)
            discriminator.train_on_batch(X_batch_real,y_train_real*(1-smooth))
            discriminator.train_on_batch(X_batch_fake,y_train_fake)
            # train the generator via GAN
            trainable(discriminator,False)
            gan.train_on_batch(batch_latent_samples,y_train_real)
        # evaluate
        X_valid_real=X_test_real[np.random.choice(len(X_test_real),
                                                  valid_size,replace=False)]
        valid_latent_samples=latent_samples(valid_size,sample_size)
        X_valid_fake=generator.predict_on_batch(valid_latent_samples)
        d_loss=discriminator.test_on_batch(X_valid_real,y_valid_real)
        d_loss+=discriminator.test_on_batch(X_valid_fake,y_valid_fake)
        g_loss=gan.test_on_batch(valid_latent_samples,y_valid_real) 
        losses.append((d_loss,g_loss))
        print("Epoch:{:>3}/{} | Discriminator Loss:{:>7.4f} | Generator Loss:{:>7.4f}"\
              .format(e+1,epochs,d_loss,g_loss))        
        if show_details and g_loss<4.5 and e>10: display_images(X_valid_fake)           
    if show_details: display_loss(losses)        
    return generator

dcgan_train(g_learning_rate=.0001,g_beta_1=.8, 
            d_learning_rate=.001,d_beta_1=.8, 
            leaky_alpha=.01);