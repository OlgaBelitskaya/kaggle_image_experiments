# -*- coding: utf-8 -*-
"""image-data-processing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12ra99SaEeicEG4OeUuTLscCQlp9NCukY
"""

# Commented out IPython magic to ensure Python compatibility.
# %run ../input/python-recipes/radial_gradient_header.py
# %radial_gradient_header Code Modules & Helpful Tools|24

import os,h5py,tarfile,math,PIL
import numpy as np,pandas as pd
import tensorflow as tf,pylab as pl
from tensorflow.keras import layers as tkl
from tensorflow.keras import callbacks as tkc
from tensorflow.keras.preprocessing.image import \
load_img,array_to_img,img_to_array
from tensorflow.keras.preprocessing import \
image_dataset_from_directory
from mpl_toolkits.axes_grid1.inset_locator import \
zoomed_inset_axes,mark_inset
data_url='https://raw.githubusercontent.com/'+\
         'OlgaBelitskaya/data_kitchen/main/'
file_path='../input/object-detection/resolution/'
tar_file='resolution.tgz'

# Commented out IPython magic to ensure Python compatibility.
# %radial_gradient_header Data Files|24

folder_names=['train','valid','test']
file_names=[[],[],[]]
for root,dirs,files in os.walk(file_path,topdown=True):
    for f in files:
        if '.jpg' in os.path.join(root,f):
            for i in range(3):
                if folder_names[i] in os.path.join(root,f): 
                    file_name=os.path.join(root,f)\
                    .replace(file_path,'')
                    file_names[i].append(file_name)
    for d in dirs:
        print(os.path.join(root,d),'\n',20*'<=>')
df=pd.DataFrame(file_names,index=folder_names).T
df.head()

PIL.Image.open(file_path+df['valid'][7])

# Commented out IPython magic to ensure Python compatibility.
# %radial_gradient_header Tar Archives|24

with tarfile.open(tar_file,'w:gz') as tar:
    for root,dirs,files in os.walk(file_path):
        for f in files:
            file_name=os.path.join(root,f).replace(file_path,'')
            tar.add(os.path.join(root,f),file_name)
    tar.close()

file_names=[[],[],[]]
with tarfile.open(tar_file,'r:gz') as tar:
    files=tar.getnames()
    tar.close()
for i in range(3):
    for f in files:
        if folder_names[i] in f:
            file_names[i].append(f)
df=pd.DataFrame(file_names,index=folder_names).T
df.tail()

# Commented out IPython magic to ensure Python compatibility.
# %radial_gradient_header TF Data from Tar Files|24

def scale01(img): return img/255
def process_input(input_rgb,img_size):
    input_yuv=tf.image.rgb_to_yuv(input_rgb)
    last_dimension_axis=len(input_yuv.shape)-1
    y,u,v=tf.split(input_yuv,3,axis=last_dimension_axis)
    return tf.image.resize(y,[img_size,img_size],method='area')
def process_target(input_rgb):
    input_yuv=tf.image.rgb_to_yuv(input_rgb)
    last_dimension_axis=len(input_yuv.shape)-1
    y,u,v=tf.split(input_yuv,3,axis=last_dimension_axis)
    return y

data_dir=tf.keras.utils.get_file(
    origin=data_url+tar_file,fname='resolution',
    extract=True,cache_dir='./')[:-11]
root_dir=os.path.join(data_dir,'data')
data_path=os.path.join(root_dir,'images')
test_path=os.path.join(data_path,'test')
test_paths=sorted(
    [os.path.join(test_path,fname) 
     for fname in os.listdir(test_path)
     if fname.endswith('.jpg')])
test_paths[:3]

[crop_size,upscale_factor]=[400,4]
input_size=crop_size//upscale_factor
batch_size=8
train_ds=image_dataset_from_directory(
    root_dir,batch_size=batch_size,
    image_size=(crop_size,crop_size),
    validation_split=.2,subset='training',
    seed=12,label_mode=None,)
valid_ds=image_dataset_from_directory(
    root_dir,batch_size=batch_size,
    image_size=(crop_size,crop_size),
    validation_split=.2,subset='validation',
    seed=123,label_mode=None,)
train_ds=train_ds.map(scale01)
valid_ds=valid_ds.map(scale01)
for batch in train_ds.take(1):
    for img in batch: display(array_to_img(img)); break

train_ds=train_ds.map(
    lambda x: (process_input(x,input_size),process_target(x)))
train_ds=train_ds.prefetch(buffer_size=16)
valid_ds=valid_ds.map(
    lambda x: (process_input(x,input_size),process_target(x)))
valid_ds=valid_ds.prefetch(buffer_size=16)
for batch in valid_ds.take(1):
    for img in batch[0]: display(array_to_img(img)); break
    print(10*'==> ')
    for img in batch[1]: display(array_to_img(img)); break

# Commented out IPython magic to ensure Python compatibility.
# %radial_gradient_header Usage in Super-Resolution|24

def model(upscale_factor=upscale_factor,channels=1):
    conv_args={'activation':'relu',
               'kernel_initializer':'Orthogonal',
               'padding':'same',}
    inputs=tf.keras.Input(shape=(None,None,channels))
    x=tkl.Conv2D(512,5,**conv_args)(inputs)
    x=tkl.Conv2D(96,5,**conv_args)(x)
    x=tkl.Conv2D(32,3,**conv_args)(x)
    x=tkl.Conv2D(channels*(upscale_factor**2),3,**conv_args)(x)
    outputs=tf.nn.depth_to_space(x,upscale_factor)
    return tf.keras.Model(inputs,outputs)

def display_results(img,prefix,title):
    img_array=img_to_array(img)
    img_array=img_array.astype('float32')/255
    fig,ax=pl.subplots()
    im=ax.imshow(img_array[::-1],origin='lower')
    pl.title(title)
    axins=zoomed_inset_axes(ax,2,loc=5)
    axins.imshow(img_array[::-1],origin='lower')
    x1,x2,y1,y2=200,300,100,200
    axins.set_xlim(x1,x2); axins.set_ylim(y1,y2)
    pl.yticks(visible=False); pl.xticks(visible=False)
    mark_inset(ax,axins,loc1=2,loc2=3,fc='none',ec='magenta')
    pl.savefig(str(prefix)+'-'+title+'.png')
    pl.show()
def low_resolution_img(img,upscale_factor):
    dimensions=(img.size[0]//upscale_factor,
                img.size[1]//upscale_factor)
    return img.resize(dimensions,PIL.Image.BICUBIC,)
def upscale_img(model,img):
    ycbcr=img.convert('YCbCr')
    y,cb,cr=ycbcr.split()
    y=img_to_array(y).astype('float32')/255
    input_img=np.expand_dims(y,axis=0)
    out=model.predict(input_img)
    out_img_y=out[0]*255.
    out_img_y=out_img_y.clip(0,255)
    out_img_y=out_img_y.reshape(
        (np.shape(out_img_y)[0],np.shape(out_img_y)[1]))
    out_img_y=PIL.Image.fromarray(np.uint8(out_img_y),mode='L')
    out_img_cb=cb.resize(out_img_y.size,PIL.Image.BICUBIC)
    out_img_cr=cr.resize(out_img_y.size,PIL.Image.BICUBIC)
    out_img=PIL.Image.merge(
        'YCbCr',(out_img_y,out_img_cb,out_img_cr))
    return out_img.convert('RGB')

class ESPCNCallback(tf.keras.callbacks.Callback):
    def __init__(self):
        super(ESPCNCallback,self).__init__()
        self.test_img=low_resolution_img(
            load_img(test_paths[0]),upscale_factor)
        self.mean_psnr=[]
    def on_epoch_begin(self,epoch,logs=None):
        self.psnr=[]
    def on_epoch_end(self,epoch,logs=None):
        print('mean PSNR for epoch: %.2f'%(np.mean(self.psnr)))
        if epoch%50==0:
            prediction=upscale_img(self.model,self.test_img)
            display_results(
                prediction,'epoch-'+str(epoch),'prediction')
        self.mean_psnr.append(np.mean(self.psnr))
    def on_test_batch_end(self,batch,logs=None):
        self.psnr.append(10*math.log10(1/logs['loss']))

early_stopping=tkc.EarlyStopping(
    monitor='loss',verbose=2,patience=20)
checkpoint_path='/tmp/checkpoint'
checkpoint=tkc.ModelCheckpoint(
    filepath=checkpoint_path,save_weights_only=True,
    monitor='loss',mode='min',save_best_only=True,verbose=2)
lr_reduction=tkc.ReduceLROnPlateau(
    monitor='val_loss',patience=10,verbose=2,factor=.95)

model=model(upscale_factor=upscale_factor,channels=1)
callbacks=[ESPCNCallback(),early_stopping,checkpoint,lr_reduction]
loss_fn=tf.keras.losses.MeanSquaredError()
optimizer=tf.keras.optimizers.Adam(learning_rate=.001)

epochs=1000
model.compile(optimizer=optimizer,loss=loss_fn,)
history=model.fit(
    train_ds,epochs=epochs,callbacks=callbacks,
    validation_data=valid_ds,verbose=2)
model.load_weights(checkpoint_path);

history_keys=list(history.history.keys())
pl.figure(figsize=(10,3))
pl.plot(history.history[history_keys[1]])
pl.grid(); pl.title(history_keys[1]); pl.show()
pl.figure(figsize=(10,3))
pl.plot(callbacks[0].mean_psnr)
pl.grid(); pl.title('mean psnr'); pl.show()

# Commented out IPython magic to ensure Python compatibility.
total_bicubic_psnr=0.; total_test_psnr=0.; n_img=10
for index,test_path in enumerate(test_paths[5:5+n_img]):
    img=load_img(test_path)
    lowres_input=low_resolution_img(img,upscale_factor)
    w=lowres_input.size[0]*upscale_factor
    h=lowres_input.size[1]*upscale_factor
    highres_img=img.resize((w,h))
    predict_img=upscale_img(model,lowres_input)
    lowres_img=lowres_input.resize((w,h))
    lowres_img_arr=img_to_array(lowres_img)
    highres_img_arr=img_to_array(highres_img)
    predict_img_arr=img_to_array(predict_img)
    bicubic_psnr=tf.image.psnr(
        lowres_img_arr,highres_img_arr,max_val=255)
    test_psnr=tf.image.psnr(
        predict_img_arr,highres_img_arr,max_val=255)
    total_bicubic_psnr+=bicubic_psnr
    total_test_psnr+=test_psnr
    print('PSNR of low resolution '+\
          'and high resolution is %.4f'%bicubic_psnr)
    print('PSNR of predict and high resolution is %.4f'%test_psnr)
    display_results(lowres_img,index,'low resolution')
    display_results(highres_img,index,'high resolution')
    display_results(predict_img,index,'prediction')
print('avg. PSNR of images with low resolution is %.4f'\
#       %(total_bicubic_psnr/n_img))
print('avg. PSNR of reconstructions is %.4f'\
#       %(total_test_psnr/n_img))