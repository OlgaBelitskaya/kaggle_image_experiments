# -*- coding: utf-8 -*-
"""classification-of-tomato-images-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14TCzILsGXpqugtOTg7EKJjSz76MMoOLc

[Google Colaboratory Variant](https://colab.research.google.com/drive/1pymaadPUhSm0T9N5h44ls-mAcrylfa2F)
## Code Modules & Functions
"""

!pip install --upgrade pip \
--user --quiet --no-warn-script-location
!pip install --upgrade neural_structured_learning \
--user --quiet --no-warn-script-location

import warnings; warnings.filterwarnings('ignore')
import tensorflow_hub as th,tensorflow as tf
import neural_structured_learning as nsl
import os,pandas as pd,numpy as np
import seaborn as sn,pylab as pl
import tensorflow.keras.callbacks as tkc,\
tensorflow.keras.layers as tkl
import tensorflow.keras.preprocessing.image as tkimg

def images2array(files_path,img_size,
                 preprocess=False,grayscale=False):
    files_list=sorted(os.listdir(files_path))
    n,img_array=len(files_list),[]
    for i in range(n):
        if i%round(.1*n)==0:
            print('=>',end='',flush=True)
        img_path=files_path+files_list[i]
        if preprocess:
            img=tkimg.load_img(
                img_path,grayscale=grayscale)
            img=tkimg.img_to_array(img)
            img=tkimg.smart_resize(
                img,(img_size,img_size))
        else:
            img=tkimg.load_img(
                img_path,target_size=(img_size,img_size))
            img=tkimg.img_to_array(img)
        img=np.expand_dims(img,axis=0)/255
        img_array.append(img)
    return np.array(np.vstack(img_array),
                    dtype='float32')
def labels2array(files_path):
    files_list=sorted(os.listdir(files_path))
    files_split=np.array([el.split('_') 
                          for el in files_list])
    num_labels=files_split.shape[1]-1
    labels=[files_split[:,i] 
            for i in range(num_labels)]
    labels=np.array(labels).astype('int32')
    for i in range(num_labels):
        label_set=list(set(labels[i]))
        replace_dict=\
        dict(zip(label_set,
                 list(range(len(label_set)))))
        labels[i]=[replace_dict.get(x,x) 
                   for x in labels[i]]
    return labels

"""## Data"""

files_path='../input/tomato-cultivars/'
img_size=160
names=[['Kumato','Beefsteak','Tigerella',
        'Roma','Japanese Black Trifele',
        'Yellow Pear','Sun Gold','Green Zebra',
        'Cherokee Purple','Oxheart','Blue Berries',
        'San Marzano','Banana Legs',
        'German Orange Strawberry','Supersweet 100']]
images=images2array(files_path,img_size)
labels=labels2array(files_path)
N=images.shape[0]; n=int(.1*N)
shuffle_ids=np.arange(N)
np.random.RandomState(12).shuffle(shuffle_ids)
images=images[shuffle_ids]
labels=np.array([labels[i][shuffle_ids]
                 for i in range(labels.shape[0])])
x_test,x_valid,x_train=\
images[:n],images[n:2*n],images[2*n:]
y_test,y_valid,y_train=\
labels[:,:n],labels[:,n:2*n],labels[:,2*n:]

df=pd.DataFrame(
    [[x_train.shape,x_valid.shape,x_test.shape],
     [x_train.dtype,x_valid.dtype,x_test.dtype],
     [y_train.shape,y_valid.shape,y_test.shape],
     [y_train.dtype,y_valid.dtype,y_test.dtype]],
    columns=['train','valid','test'],
    index=['image shape','image type',
           'label shape','label type'])
df

cmap='tab20'
idx=['labels %d'%(i+1) for i in range(labels.shape[0])]
df=pd.DataFrame(labels,index=idx).T
for i in range(labels.shape[0]):
    df['name %d'%(i+1)]=\
    [names[i][l] for l in labels[i]]
fig=pl.figure(figsize=(9,3))    
for i in range(labels.shape[0]):
    ax=fig.add_subplot(labels.shape[0],1,i+1)
    sn.countplot(y='name %s'%(i+1),data=df,
                 palette=cmap,alpha=.5,ax=ax)
pl.show()

"""## NN Examples"""

def premodel(pix,den,mh,lbl,activ,loss):
    model=tf.keras.Sequential([
        tkl.Input((pix,pix,int(3)),name='input'),
        th.KerasLayer(mh,trainable=True),
        tkl.Flatten(),
        tkl.Dense(den,activation='relu'),
        tkl.Dropout(rate=.5),
        tkl.Dense(lbl,activation=activ)])
    model.compile(optimizer='adam',metrics=['accuracy'],loss=loss)
    return model
def cb(fw):
    early_stopping=tkc.EarlyStopping(
        monitor='val_loss',patience=int(20),verbose=int(2))
    checkpointer=tkc.ModelCheckpoint(
        filepath=fw,verbose=int(2),save_weights_only=True,
        monitor='val_accuracy',mode='max',save_best_only=True)
    lr_reduction=tkc.ReduceLROnPlateau(
        monitor='val_loss',verbose=int(2),patience=int(5),factor=.8)
    return [checkpointer,early_stopping,lr_reduction]

fw='/tmp/checkpoint'
[handle_base,pixels]=['inception_v3',160]
mhandle='https://tfhub.dev/google/imagenet/{}/classification/4'\
.format(handle_base)

model=premodel(pixels,1024,mhandle,15,'softmax',
               'sparse_categorical_crossentropy')
history=model.fit(
    x=x_train,y=np.squeeze(y_train),
    batch_size=32,epochs=70,callbacks=cb(fw),
    validation_data=(x_valid,np.squeeze(y_valid)))

model.load_weights(fw)
model.evaluate(x_test,np.squeeze(y_test))

[handle_base,pixels]=['mobilenet_v2_100_160',160]
mhandle='https://tfhub.dev/google/imagenet/{}/classification/4'\
.format(handle_base)

model=premodel(pixels,1024,mhandle,15,'softmax',
               'sparse_categorical_crossentropy')
history=model.fit(
    x=x_train,y=np.squeeze(y_train),
    batch_size=32,epochs=70,callbacks=cb(fw),
    validation_data=(x_valid,np.squeeze(y_valid)))

model.load_weights(fw)
model.evaluate(x_test,np.squeeze(y_test))

batch_size=64; img_size=x_train.shape[1]; epochs=30
base_model=tf.keras.Sequential([
    tkl.Input((img_size,img_size,3),name='input'),
    tkl.Conv2D(32,(5,5),padding='same'),
    tkl.Activation('relu'),
    tkl.MaxPooling2D(pool_size=(2,2)),
    tkl.Dropout(.25),
    tkl.Conv2D(196,(5,5)),
    tkl.Activation('relu'),    
    tkl.MaxPooling2D(pool_size=(2,2)),
    tkl.Dropout(.25),
    tkl.GlobalMaxPooling2D(),    
    tkl.Dense(512),
    tkl.Activation('relu'),
    tkl.Dropout(.25),
    tkl.Dense(128),
    tkl.Activation('relu'),
    tkl.Dropout(.25),
    tkl.Dense(15,activation='softmax')])
adv_config=nsl.configs\
.make_adv_reg_config(multiplier=.2,adv_step_size=.05)
adv_model=nsl.keras\
.AdversarialRegularization(base_model,adv_config=adv_config)
adv_model.compile(
    optimizer='adam',metrics=['accuracy'],
    loss='sparse_categorical_crossentropy')

train=tf.data.Dataset.from_tensor_slices(
    {'input':x_train,'label':np.squeeze(y_train)})\
     .batch(batch_size)
valid=tf.data.Dataset.from_tensor_slices(
    {'input':x_valid,'label':np.squeeze(y_valid)})\
     .batch(batch_size)
valid_steps=x_valid.shape[0]//batch_size
adv_model.fit(train,validation_data=valid,verbose=2,
              validation_steps=valid_steps,epochs=epochs)

adv_model.evaluate(
    {'input':x_test,'label':np.squeeze(y_test)})